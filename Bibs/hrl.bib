@article{schulman2017proximal,
  title =        {Proximal policy optimization algorithms},
  author =       {Schulman, John and Wolski, Filip and Dhariwal,
                  Prafulla and Radford, Alec and Klimov, Oleg},
  journal =      {arXiv preprint arXiv:1707.06347},
  year =         2017
}

@inproceedings{schulman2015trust,
  title =        {Trust region policy optimization},
  author =       {Schulman, John and Levine, Sergey and Abbeel,
                  Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle =    {International conference on machine learning},
  pages =        {1889--1897},
  year =         2015
}

@inproceedings{bacon2017option,
  title =        {The option-critic architecture},
  author =       {Bacon, Pierre-Luc and Harb, Jean and Precup,
                  Doina},
  booktitle =    {Thirty-First AAAI Conference on Artificial
                  Intelligence},
  year =         2017
}

@article{sutton1999between,
  title =        {Between MDPs and semi-MDPs: A framework for
                  temporal abstraction in reinforcement learning},
  author =       {Sutton, Richard S and Precup, Doina and Singh,
                  Satinder},
  journal =      {Artificial Intelligence},
  volume =       112,
  number =       {1-2},
  pages =        {181--211},
  year =         1999,
  publisher =    {Elsevier}
}

@article{hyun2019disentangling,
  title =        {Disentangling Options with Hellinger Distance
                  Regularizer},
  author =       {Hyun, Minsung and Choi, Junyoung and Kwak,
                  Nojun},
  journal =      {arXiv preprint arXiv:1904.06887},
  year =         2019
}

@inproceedings{masson2016reinforcement,
  title =        {Reinforcement learning with parameterized
                  actions},
  author =       {Masson, Warwick and Ranchod, Pravesh and
                  Konidaris, George},
  booktitle =    {Thirtieth AAAI Conference on Artificial
                  Intelligence},
  year =         2016
}

@article{ye2019mastering,
  title =        {Mastering Complex Control in MOBA Games with
                  Deep Reinforcement Learning},
  author =       {Ye, Deheng and Liu, Zhao and Sun, Mingfei and
                  Shi, Bei and Zhao, Peilin and Wu, Hao and Yu,
                  Hongsheng and Yang, Shaojie and Wu, Xipeng and
                  Guo, Qingwei and others},
  journal =      {arXiv preprint arXiv:1912.09729},
  year =         2019
}

@inproceedings{ciosek2018expected,
  title =        {Expected policy gradients},
  author =       {Ciosek, Kamil and Whiteson, Shimon},
  booktitle =    {Thirty-Second AAAI Conference on Artificial
                  Intelligence},
  year =         2018
}

@book{pearl2014probabilistic,
  title =        {Probabilistic reasoning in intelligent systems:
                  networks of plausible inference},
  author =       {Pearl, Judea},
  year =         2014,
  publisher =    {Elsevier}
}

@inproceedings{konda2000actor,
  title =        {Actor-critic algorithms},
  author =       {Konda, Vijay R and Tsitsiklis, John N},
  booktitle =    {Advances in neural information processing
                  systems},
  pages =        {1008--1014},
  year =         2000
}

@article{hinton2018matrix,
  title =        {Matrix capsules with EM routing},
  author =       {Hinton, Geoffrey E and Sabour, Sara and Frosst,
                  Nicholas},
  year =         2018
}

@article{schulman2015high,
  title =        {High-dimensional continuous control using
                  generalized advantage estimation},
  author =       {Schulman, John and Moritz, Philipp and Levine,
                  Sergey and Jordan, Michael and Abbeel, Pieter},
  journal =      {arXiv preprint arXiv:1506.02438},
  year =         2015
}

@book{sutton2018reinforcement,
  title =        {Reinforcement learning: An introduction},
  author =       {Sutton, Richard S and Barto, Andrew G},
  year =         2018
}

@article{klissarov2017learnings,
  title =        {Learnings options end-to-end for continuous
                  action tasks},
  author =       {Klissarov, Martin and Bacon, Pierre-Luc and
                  Harb, Jean and Precup, Doina},
  journal =      {arXiv preprint arXiv:1712.00004},
  year =         2017
}

@inproceedings{harb2018waiting,
  title =        {When waiting is not an option: Learning options
                  with a deliberation cost},
  author =       {Harb, Jean and Bacon, Pierre-Luc and Klissarov,
                  Martin and Precup, Doina},
  booktitle =    {Thirty-Second AAAI Conference on Artificial
                  Intelligence},
  year =         2018
}

@phdthesis{eldercreating,
  title =        {Creating Algorithmic Traders with Hierarchical
                  Reinforcement Learning MSc Dissertation},
  author =       {Elder, Thomas},
  school =       {School of Informatics, University of Edinburgh}
}

@article{osada2005chq,
  title =        {CHQ: a multi-agent reinforcement learning
                  scheme for partially observable Markov decision
                  processes},
  author =       {Osada, Hiroshi and Fujita, Satoshi},
  journal =      {IEICE TRANSACTIONS on Information and Systems},
  volume =       88,
  number =       5,
  pages =        {1004--1011},
  year =         2005,
  publisher =    {The Institute of Electronics, Information and
                  Communication Engineers}
}

@inproceedings{zhang2019dac,
  title =        {DAC: The double actor-critic architecture for
                  learning options},
  author =       {Zhang, Shangtong and Whiteson, Shimon},
  booktitle =    {Advances in Neural Information Processing
                  Systems},
  pages =        {2012--2022},
  year =         2019
}

@article{machado2017eigenoption,
  title =        {Eigenoption discovery through the deep
                  successor representation},
  author =       {Machado, Marlos C and Rosenbaum, Clemens and
                  Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald
                  and Campbell, Murray},
  journal =      {arXiv preprint arXiv:1710.11089},
  year =         2017
}

@inproceedings{levy2011unified,
  title =        {Unified inter and intra options learning using
                  policy gradient methods},
  author =       {Levy, Kfir Y and Shimkin, Nahum},
  booktitle =    {European Workshop on Reinforcement Learning},
  pages =        {153--164},
  year =         2011,
  organization = {Springer}
}

@article{daniel2016probabilistic,
  title =        {Probabilistic inference for determining options
                  in reinforcement learning},
  author =       {Daniel, Christian and Van Hoof, Herke and
                  Peters, Jan and Neumann, Gerhard},
  journal =      {Machine Learning},
  volume =       104,
  number =       {2-3},
  pages =        {337--357},
  year =         2016,
  publisher =    {Springer}
}

@inproceedings{li2017infogail,
  title =        {Infogail: Interpretable imitation learning from
                  visual demonstrations},
  author =       {Li, Yunzhu and Song, Jiaming and Ermon,
                  Stefano},
  booktitle =    {Advances in Neural Information Processing
                  Systems},
  pages =        {3812--3822},
  year =         2017
}

@article{kolobov2012discovering,
  title =        {Discovering hidden structure in factored MDPs},
  author =       {Kolobov, Andrey and Weld, Daniel S and others},
  journal =      {Artificial Intelligence},
  volume =       189,
  pages =        {19--47},
  year =         2012,
  publisher =    {Elsevier}
}
